{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Embedding, TextVectorization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import nltk # for natural language processing\n",
    "from nltk.corpus import stopwords # for removing english stopwords\n",
    "from nltk.stem import WordNetLemmatizer # for term stemming\n",
    "from prettytable import PrettyTable\n",
    "import sklearn # for predictive data analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "from IPython.core.display import SVG\n",
    "from IPython.core.interactiveshell import InteractiveShell # to modify Jupyter notebook configuration\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # so that all outputs in a cell are returned (instead of last instance)\n",
    "\n",
    "# #Supress default INFO logging\n",
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fake and true news CSVs into Pandas dataframes\n",
    "true_news = pd.read_csv('True.csv') \n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Add column for fake/true target (true == 1, false == 0)\n",
    "true_news['target'] = 1\n",
    "fake_news['target'] = 0\n",
    "\n",
    "# True and Fake news value counts - are they balanced?\n",
    "print(\"Compare number of observations in true news and fake news data frames\")\n",
    "true_news['target'].value_counts()\n",
    "print()\n",
    "fake_news['target'].value_counts()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove random rows from fake_news (n = 2064) data frame so it has same number of rows as true_news\n",
    "np.random.seed(5)\n",
    "remove_n = 2064\n",
    "drop_indices = np.random.choice(fake_news.index, remove_n, replace = False)\n",
    "fake_news = fake_news.drop(drop_indices)\n",
    "\n",
    "# Check that have same number of observations now\n",
    "print(\"True and fake datasets should have same number of samples now...\")\n",
    "true_news['target'].value_counts()\n",
    "print()\n",
    "fake_news['target'].value_counts()\n",
    "print()\n",
    "\n",
    "# Preview first and last 5 rows in datasets to ensure they imported properly\n",
    "print(\"Preview of the raw datasets to ensure they imported properly:\")\n",
    "true_news.head(-5)\n",
    "print()\n",
    "fake_news.head(-5)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine true_news and fake_news data frames into one\n",
    "dfs = [true_news, fake_news]\n",
    "news_data = pd.concat(dfs)\n",
    "\n",
    "# Concatenate text columns and isolate only relevant columns for analysis (i.e., text and target)\n",
    "news_data['text'] = news_data['title'] + ' ' + news_data['text']\n",
    "news_data = news_data[['text', 'target']]\n",
    "\n",
    "# Check that binary values were assigned correctly\n",
    "print(\"Dimensions of data frame that will be cleaned:\")\n",
    "news_data.shape # data frame dimensions\n",
    "print()\n",
    "\n",
    "print(\"First and last five rows of pre-cleaned concatenated dataset:\")\n",
    "news_data.head(-5) # first 5 and last 5 rows\n",
    "print()\n",
    "\n",
    "print(\"Null values by column:\")\n",
    "news_data.isnull().sum() # check for null values\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate WordNetLemmatizer() -- reduce words to their roots\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Download multilingual Wordnet data from OMW\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# List of english stopwords\n",
    "nltk.download('stopwords') \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Download english dictionary ('wordnet')\n",
    "nltk.download('wordnet');\n",
    "\n",
    "# Download pre-trained GloVe embeddings using the following commands: \n",
    "# \"wget http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "# \"unzip -q glove.6B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(row):\n",
    "    row = row.lower() # convert text into lowercase\n",
    "    row = re.sub('[^a-zA-Z]', ' ', row) # remove number and special characters using regex (keep words only)\n",
    "    token = row.split() # split the data into tokens\n",
    "    news = [wnl.lemmatize(word) for word in token if not word in stop_words] # lemmatize the words and remove any stopwords (e.g., a, an, the, etc.)\n",
    "    row_clean = [word for word in news if len(word) >= 3] # only keep words greater than or equal to length of 3\n",
    "    cleaned_news = ' '.join(row_clean) # join all tokenized words with space in between \n",
    "    \n",
    "    return cleaned_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data - might take a couple minutes to run.\n",
    "news_data['text'] = news_data['text'].apply(lambda x : data_cleaning(x)) # 'text' column gets cleaned\n",
    "print(\"First and last five rows after cleaning the data:\")\n",
    "news_data.head(-5) # check that cleaning went as planned\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"Null values by column:\")\n",
    "news_data.isnull().sum() # want zero null values\n",
    "print()\n",
    "\n",
    "# Check number unique values in each column\n",
    "print(\"Unique values by column:\")\n",
    "news_data.nunique() # number unique values in each column\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the target and text features\n",
    "target = news_data['target'] # target values\n",
    "text_dataset = news_data['text'] # predictor text features\n",
    "\n",
    "# Check first and last 5 rows of target and text datasets\n",
    "target.head(-5)\n",
    "text_dataset.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test subsets\n",
    "train_data, test_data, train_target, test_target = train_test_split(text_dataset, target, random_state = 5, train_size = 0.80)\n",
    "\n",
    "# Check the split from into training and testing datasets\n",
    "train_data.head(-5)\n",
    "test_data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text_dataset, train_data, and test_data data frames to a tensor\n",
    "train_data = tf.convert_to_tensor(train_data, dtype = tf.string) # train data\n",
    "test_data = tf.convert_to_tensor(test_data, dtype = tf.string) # test data\n",
    "train_target = tf.convert_to_tensor(train_target, dtype = tf.int32) # train data\n",
    "test_target = tf.convert_to_tensor(test_target, dtype = tf.int32) # test data\n",
    "text_dataset = tf.convert_to_tensor(text_dataset, dtype = tf.string) # text dataset (train and test combined) for full vocabulary\n",
    "\n",
    "# Double check conversion to tensor\n",
    "test_data\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different values to try for TfidVectorizer max_features (i.e., top key words)\n",
    "key_words = [100, 200, 500, 1000, 5000] # How many of the top key words to keep - iterate over list\n",
    "n_grams = [(1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3)] # ngram_range dictates if we keep 1 word (1, 1), 1 or 2 words (1, 2) etc.\n",
    "max_len = [300, 600] # sequence length to pad the outputs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams parameter defines phrase length and iterates over n_grams list.\n",
    "# (1, 1) keeps single words only, (3,3) keeps three word phrases,\n",
    "# (1, 3) keeps one, two, or three word phrases in top n words and so on.\n",
    "for ng in n_grams:\n",
    "\n",
    "    # Max tokens parameter chooses the top n words (iterate over key_words list)\n",
    "    for kw in key_words:\n",
    "        \n",
    "        # output_sequence_length parameter defines the sequence length to pad the outputs to\n",
    "        # iterates over values in max_len list.\n",
    "        for ml in max_len:\n",
    "\n",
    "            # Create the vocabulary index. \n",
    "            vectorizer = tf.keras.layers.TextVectorization(\n",
    "                max_tokens = kw, # number of top key words to keep\n",
    "                standardize = None, # data already cleaned above\n",
    "                split = 'whitespace', # already split tokens during data cleaning\n",
    "                ngrams = ng, # range of token string lengths to keep in max_tokens (unigram, bigram, trigram)\n",
    "                output_mode = 'int', # outputs one integer index per split string token\n",
    "                output_sequence_length = ml, # if set, output will have its time (length) dimension padded to exactly the specified value\n",
    "                pad_to_max_tokens = False, # not valid argument for integer outputs\n",
    "                vocabulary = None, # optional, the adapt layer below handle's this step better\n",
    "                idf_weights = None, # not valid argument for integer outputs\n",
    "                sparse = False, # not applicable argument for integer outputs\n",
    "                ragged = False, # false cause you want each sequence shrunk or padded to the same output_sequence_length\n",
    "            )\n",
    "\n",
    "            # Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "            # dataset to create the vocabulary. You don't have to batch, but for large\n",
    "            # datasets this means we're not keeping spare copies of the dataset.\n",
    "            vectorizer.adapt(text_dataset)\n",
    "            \n",
    "            # Retrieve the top 10 words from vectorized vocabulary\n",
    "            vectorizer.get_vocabulary()[:10]\n",
    "\n",
    "            # Create a dictionary mapping words to their indices\n",
    "            voc = vectorizer.get_vocabulary()\n",
    "            word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "            # Make a dictionary mapping strings to their NumPy vector representation in gloVe:\n",
    "            data_dir = 'E:/Zack/School/Classes/Summer 22/CSC 7333 - Machine Learning/Group Project/glove.6B.100d.txt'\n",
    "\n",
    "            embeddings_index = {}\n",
    "            with open(data_dir, encoding = \"utf8\") as f:\n",
    "                for line in f:\n",
    "                    word, coefs = line.split(maxsplit = 1)\n",
    "                    coefs = np.fromstring(coefs, \"f\", sep = \" \")\n",
    "                    embeddings_index[word] = coefs\n",
    "\n",
    "            print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "            print()\n",
    "\n",
    "            # Prepare a corresponding embedding matrix to use in the model's Embedding layer\n",
    "            num_tokens = len(voc) + 2 # plus 2 because 0 is reserved for padding and 1 is reserved for tokens not in vocab\n",
    "            embedding_dim = 100\n",
    "            hits = 0\n",
    "            misses = 0\n",
    "\n",
    "            # Prepare embedding matrix\n",
    "            embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "            for word, i in word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # Words not found in embedding index will be all-zeros.\n",
    "                    # This includes the representation for \"padding\" and tokens not in vocabulary index\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    hits += 1\n",
    "                else:\n",
    "                    misses += 1\n",
    "            print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "            print()\n",
    "            \n",
    "            # Create the model that uses the vectorized text layer\n",
    "            model = Sequential()\n",
    "\n",
    "            # Start by creating an explicit input layer. It needs to have a shape of\n",
    "            # (1,) (because we need to guarantee that there is exactly one string\n",
    "            # input per batch), and the dtype needs to be 'string'.\n",
    "            model.add(tf.keras.Input(shape = (1,), dtype = tf.string))\n",
    "\n",
    "            # The first layer in our model is the vectorization layer. After this layer,\n",
    "            # we have a tensor of shape (batch_size, max_len) containing vocab indices\n",
    "            model.add(vectorizer)\n",
    "\n",
    "            # Now, the model can map strings to integers, and you can add an embedding\n",
    "            # layer to map these integers to learned embeddings.\n",
    "            model.add(Embedding(\n",
    "                num_tokens, # kw (or voc) + 2\n",
    "                embedding_dim, # 100 dimension matrix\n",
    "                input_length = ml, # size of the padded sequence\n",
    "                embeddings_initializer = keras.initializers.Constant(embedding_matrix), # intialize based on your embedding matrix\n",
    "                trainable = False # set to false so you don't update embedding during training\n",
    "            ))\n",
    "\n",
    "            # Finish building the model\n",
    "            model.add(Conv1D(filters = 128, kernel_size = 5, padding = 'same', activation = 'relu')) # extract local features using 128 filters with kernel size of 5 and default ReLU activiation function\n",
    "            model.add(MaxPooling1D(pool_size = 2)) # pool large vector features from above CNN layer with window size of 2--this downsamples the feature vectors/parameters\n",
    "            model.add(LSTM(32)) # this is your RNN (long short-term memory (LSTM)) layer; the pooled features from above layer are the input w/ default hyperbolic tangent activation\n",
    "            model.add(Dense(1, activation = 'sigmoid')) # this layer classifies the trained vector features and shrinks the output dimension to 1, which corresponds to classification label. \n",
    "            model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # train the model using batch size 64 across 10 epochs using adaptive moment estimation (ADAM) to define learning rate in each epoch\n",
    "\n",
    "            # CNN-RNN hybrid model structure: \n",
    "            print(f\"CNN-RNN hybrid model structure using top {kw} words, an n-gram range of {ng}, and padded length of {ml} is:\") \n",
    "            print(model.summary())\n",
    "            print()\n",
    "\n",
    "            # Create variable for storing start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Save model weights \n",
    "            filepath = f\"best_weights_cnn-rnn_{kw}words_{ng}range_{ml}maxLength.tf\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max', save_weights_only = True)\n",
    "            callbacks_list = [checkpoint]\n",
    "            history = model.fit(train_data, train_target, epochs = 5, batch_size = 64, verbose = 1, callbacks = callbacks_list, validation_split = 0.2) # fit the model on training data\n",
    "\n",
    "            # Evaluate the model\n",
    "            scores = model.evaluate(test_data, test_target, verbose = 1)\n",
    "            loss = (scores[0]) # store best loss value\n",
    "            accuracy = (scores[1]*100) # store best accuracy value\n",
    "            print(f\"CNN-RNN hybrid model accuracy using top {kw}, an n-gram range of {ng}, and padded length of {ml} is: {accuracy:.2f}%\")\n",
    "            print()\n",
    "\n",
    "            # Create variable for storing current time\n",
    "            # Subtract start time from current time to get runtime\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            print(f\"Model training and evaluation time using top {kw} words, an n-gram range of {ng}, and padded length of {ml} is: {elapsed_time:.2f} seconds\")\n",
    "            print()\n",
    "\n",
    "            # Training and validation accuracy\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.plot(history.history['accuracy']) # training accuracy\n",
    "            plt.plot(history.history['val_accuracy']) # validation accuracy\n",
    "            plt.title(f'CNN-RNN Model Accuracy: n-gram range {ng},\\n{kw} top words, and padded length of {ml}')\n",
    "            plt.legend(['Training', 'Validation'], loc = 'center right')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.xticks([0, 1, 2, 3, 4])\n",
    "            text_box = AnchoredText(f\"Accuracy: {accuracy:.2f}%\", loc = 'lower right', frameon = False, pad = 0.5)\n",
    "            plt.setp(text_box.patch, facecolor = 'white', alpha = 0.5)\n",
    "            ax.add_artist(text_box)\n",
    "            plt.show();\n",
    "\n",
    "            # Training and validation loss\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.plot(history.history['loss']) # training accuracy\n",
    "            plt.plot(history.history['val_loss']) # validation accuracy\n",
    "            plt.title(f'CNN-RNN Model Loss: n-gram range {ng},\\n{kw} top words, and padded length of {ml}')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.xticks([0, 1, 2, 3, 4])\n",
    "            plt.legend(['Training', 'Validation'], loc = 'center right')\n",
    "            text_box = AnchoredText(f\"Loss: {loss:.4f}\", loc = 'upper right', frameon = False, pad = 0.5)\n",
    "            plt.setp(text_box.patch, facecolor = 'white', alpha = 0.5)\n",
    "            ax.add_artist(text_box)\n",
    "            plt.show();\n",
    "\n",
    "            # # Create empty dictionary to append model metrics to\n",
    "            modelMetrics = [('model', filepath), ('n_grams', ng), ('key_words', kw), ('padded_length', ml), ('loss', loss), ('accuracy', accuracy), ('elapsed_time', elapsed_time)]\n",
    "            metrics_dict = defaultdict(list)\n",
    "            for k, v in modelMetrics:\n",
    "                metrics_dict[k].append(v)\n",
    "            \n",
    "            updatedMetrics = []\n",
    "            updatedMetrics = updatedMetrics.append(metrics_dict)\n",
    "\n",
    "            print(f\"Updated model metrics for top {kw} words, an n-gram range of {ng}, and padded length of {ml} are:\")\n",
    "            print(tabulate(updatedMetrics, headers = 'keys'))\n",
    "            print();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb41745365b0f6cd342db2064230ca33a3bbb95e73a9c1a8b502f45a833dbf8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
